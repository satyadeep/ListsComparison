{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971d0394",
   "metadata": {},
   "source": [
    "# Processing Overlay for Custom Comparison Operations\n",
    "\n",
    "This notebook implements a loading/processing overlay system for custom comparison operations, allowing users to visualize progress during potentially time-consuming comparison tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b5014",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any, Callable, Union, Tuple\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ce46f",
   "metadata": {},
   "source": [
    "## Set Up Data Loading Functions\n",
    "\n",
    "These functions provide visual feedback during data loading operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f16b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_progress(data_source, total_items=100, simulate_delay=True):\n",
    "    \"\"\"\n",
    "    Load data with a progress bar visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_source : str or pd.DataFrame\n",
    "        Source of data - can be a filepath or a DataFrame\n",
    "    total_items : int\n",
    "        Number of items to process (for simulation purposes)\n",
    "    simulate_delay : bool\n",
    "        Whether to simulate loading delay for demonstration\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        The loaded data\n",
    "    \"\"\"\n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(total=total_items)\n",
    "    \n",
    "    # Placeholder for the data\n",
    "    data = None\n",
    "    \n",
    "    # If data_source is a string, assume it's a filepath\n",
    "    if isinstance(data_source, str):\n",
    "        # Simulate loading from file\n",
    "        for i in range(total_items):\n",
    "            if simulate_delay:\n",
    "                time.sleep(random.uniform(0.01, 0.05))  # Simulate variable loading time\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f\"Loading item {i+1}/{total_items}\")\n",
    "        \n",
    "        # Generate sample data if we're just simulating\n",
    "        data = pd.DataFrame({\n",
    "            'id': range(1, total_items+1),\n",
    "            'value_a': np.random.rand(total_items) * 100,\n",
    "            'value_b': np.random.rand(total_items) * 100,\n",
    "            'category': np.random.choice(['A', 'B', 'C', 'D'], total_items)\n",
    "        })\n",
    "    \n",
    "    # If data_source is already a DataFrame\n",
    "    elif isinstance(data_source, pd.DataFrame):\n",
    "        data = data_source\n",
    "        total_items = len(data)\n",
    "        \n",
    "        # Simulate processing the existing DataFrame\n",
    "        for i in range(total_items):\n",
    "            if simulate_delay:\n",
    "                time.sleep(random.uniform(0.01, 0.03))\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f\"Processing item {i+1}/{total_items}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(f\"âœ… Data loading complete - {total_items} items processed\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_datasets(sources, names=None):\n",
    "    \"\"\"\n",
    "    Load multiple datasets with progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sources : list\n",
    "        List of data sources\n",
    "    names : list, optional\n",
    "        Names for each dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of loaded datasets\n",
    "    \"\"\"\n",
    "    if names is None:\n",
    "        names = [f\"dataset_{i}\" for i in range(len(sources))]\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for i, (source, name) in enumerate(zip(sources, names)):\n",
    "        print(f\"Loading dataset {i+1}/{len(sources)}: {name}\")\n",
    "        datasets[name] = load_data_with_progress(\n",
    "            source, \n",
    "            total_items=random.randint(50, 150),  # Simulate different dataset sizes\n",
    "            simulate_delay=True\n",
    "        )\n",
    "        print(f\"Dataset '{name}' loaded with {len(datasets[name])} records\\n\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4acf6",
   "metadata": {},
   "source": [
    "## Create Processing Overlay\n",
    "\n",
    "Implement visual overlay components to display during processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingOverlay:\n",
    "    \"\"\"A class to manage processing overlays with various visual indicators\"\"\"\n",
    "    \n",
    "    def __init__(self, title=\"Processing...\"):\n",
    "        \"\"\"Initialize the overlay with default components\"\"\"\n",
    "        self.title = title\n",
    "        self.progress_bar = None\n",
    "        self.status_text = None\n",
    "        self.output_area = None\n",
    "        self.is_active = False\n",
    "    \n",
    "    def create_spinner_overlay(self):\n",
    "        \"\"\"Create a spinner-style overlay\"\"\"\n",
    "        # Create output area for the overlay\n",
    "        self.output_area = widgets.Output()\n",
    "        \n",
    "        # Create status text widget\n",
    "        self.status_text = widgets.HTML(\n",
    "            value=f\"<h3>{self.title}</h3><p>Operation in progress...</p>\"\n",
    "        )\n",
    "        \n",
    "        # Create a layout for centering content\n",
    "        center_layout = widgets.Layout(\n",
    "            display='flex',\n",
    "            flex_flow='column',\n",
    "            align_items='center',\n",
    "            justify_content='center',\n",
    "            width='100%',\n",
    "            height='100%'\n",
    "        )\n",
    "        \n",
    "        # Create a container with the centered layout\n",
    "        container = widgets.VBox([\n",
    "            self.status_text,\n",
    "            widgets.HTML(value='<div class=\"spinner-border text-primary\" role=\"status\"></div>'),\n",
    "        ], layout=center_layout)\n",
    "        \n",
    "        # Display the container in the output area\n",
    "        with self.output_area:\n",
    "            display(container)\n",
    "        \n",
    "        return self.output_area\n",
    "    \n",
    "    def create_progress_overlay(self, total=100):\n",
    "        \"\"\"Create a progress bar overlay\"\"\"\n",
    "        # Create output area for the overlay\n",
    "        self.output_area = widgets.Output()\n",
    "        \n",
    "        # Create status text widget\n",
    "        self.status_text = widgets.HTML(\n",
    "            value=f\"<h3>{self.title}</h3><p>Operation in progress...</p>\"\n",
    "        )\n",
    "        \n",
    "        # Create progress bar\n",
    "        self.progress_bar = widgets.IntProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=total,\n",
    "            description='Processing:',\n",
    "            bar_style='info',\n",
    "            orientation='horizontal'\n",
    "        )\n",
    "        \n",
    "        # Create a layout for centering content\n",
    "        center_layout = widgets.Layout(\n",
    "            display='flex',\n",
    "            flex_flow='column',\n",
    "            align_items='center',\n",
    "            justify_content='center',\n",
    "            width='100%',\n",
    "            margin='10px 0px'\n",
    "        )\n",
    "        \n",
    "        # Create a container with the centered layout\n",
    "        container = widgets.VBox([\n",
    "            self.status_text,\n",
    "            self.progress_bar\n",
    "        ], layout=center_layout)\n",
    "        \n",
    "        # Display the container in the output area\n",
    "        with self.output_area:\n",
    "            display(container)\n",
    "        \n",
    "        return self.output_area\n",
    "    \n",
    "    def show(self, overlay_type='progress', total=100):\n",
    "        \"\"\"Show the overlay\"\"\"\n",
    "        if overlay_type == 'progress':\n",
    "            display(self.create_progress_overlay(total))\n",
    "        else:  # spinner\n",
    "            display(self.create_spinner_overlay())\n",
    "        self.is_active = True\n",
    "    \n",
    "    def update_progress(self, value, message=None):\n",
    "        \"\"\"Update the progress bar value and optionally the message\"\"\"\n",
    "        if self.progress_bar is not None:\n",
    "            self.progress_bar.value = value\n",
    "        \n",
    "        if message is not None and self.status_text is not None:\n",
    "            self.status_text.value = f\"<h3>{self.title}</h3><p>{message}</p>\"\n",
    "    \n",
    "    def update_status(self, message):\n",
    "        \"\"\"Update just the status message\"\"\"\n",
    "        if self.status_text is not None:\n",
    "            self.status_text.value = f\"<h3>{self.title}</h3><p>{message}</p>\"\n",
    "    \n",
    "    def hide(self):\n",
    "        \"\"\"Hide the overlay\"\"\"\n",
    "        if self.output_area is not None:\n",
    "            self.output_area.clear_output()\n",
    "        self.is_active = False\n",
    "\n",
    "# Example of usage\n",
    "def demo_processing_overlay():\n",
    "    overlay = ProcessingOverlay(\"Data Processing Demo\")\n",
    "    overlay.show(overlay_type='progress', total=100)\n",
    "    \n",
    "    for i in range(101):\n",
    "        time.sleep(0.05)  # Simulate processing time\n",
    "        overlay.update_progress(i, f\"Processing item {i}/100\")\n",
    "    \n",
    "    overlay.update_status(\"Processing complete!\")\n",
    "    time.sleep(1)\n",
    "    overlay.hide()\n",
    "    print(\"Processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061abec",
   "metadata": {},
   "source": [
    "## Implement Custom Comparison Logic\n",
    "\n",
    "Here we develop the core comparison functionality that will be enhanced with the processing overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataComparator:\n",
    "    \"\"\"Class to handle different types of data comparisons with visual feedback\"\"\"\n",
    "    \n",
    "    def __init__(self, datasets=None):\n",
    "        \"\"\"Initialize with optional datasets\"\"\"\n",
    "        self.datasets = datasets or {}\n",
    "        self.comparison_results = {}\n",
    "        self.overlay = ProcessingOverlay(\"Comparison in Progress\")\n",
    "    \n",
    "    def add_dataset(self, name, data):\n",
    "        \"\"\"Add a dataset to the comparator\"\"\"\n",
    "        self.datasets[name] = data\n",
    "        print(f\"Added dataset '{name}' with {len(data)} rows\")\n",
    "    \n",
    "    def column_distribution_comparison(self, dataset1_name, dataset2_name, column, bins=10):\n",
    "        \"\"\"\n",
    "        Compare the distribution of values in a specific column between two datasets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset1_name, dataset2_name : str\n",
    "            Names of datasets to compare\n",
    "        column : str\n",
    "            Column to compare\n",
    "        bins : int\n",
    "            Number of bins for histogram comparison\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Comparison results\n",
    "        \"\"\"\n",
    "        # Get the datasets\n",
    "        dataset1 = self.datasets.get(dataset1_name)\n",
    "        dataset2 = self.datasets.get(dataset2_name)\n",
    "        \n",
    "        if dataset1 is None or dataset2 is None:\n",
    "            raise ValueError(\"One or both datasets not found\")\n",
    "        \n",
    "        if column not in dataset1.columns or column not in dataset2.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in one or both datasets\")\n",
    "        \n",
    "        # Show processing overlay\n",
    "        self.overlay.show(overlay_type='progress', total=100)\n",
    "        self.overlay.update_status(f\"Comparing distributions for column '{column}'\")\n",
    "        \n",
    "        # Extract column data\n",
    "        data1 = dataset1[column].values\n",
    "        data2 = dataset2[column].values\n",
    "        \n",
    "        # Simulate processing time\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.update_progress(20, \"Extracting column data...\")\n",
    "        \n",
    "        # Calculate histograms\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.update_progress(40, \"Calculating histograms...\")\n",
    "        \n",
    "        hist1, edges1 = np.histogram(data1, bins=bins)\n",
    "        hist2, edges2 = np.histogram(data2, bins=bins)\n",
    "        \n",
    "        # Normalize histograms for comparison\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.update_progress(60, \"Normalizing distributions...\")\n",
    "        \n",
    "        hist1_norm = hist1 / hist1.sum() if hist1.sum() > 0 else hist1\n",
    "        hist2_norm = hist2 / hist2.sum() if hist2.sum() > 0 else hist2\n",
    "        \n",
    "        # Calculate difference metrics\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.update_progress(80, \"Calculating difference metrics...\")\n",
    "        \n",
    "        # Absolute difference\n",
    "        abs_diff = np.abs(hist1_norm - hist2_norm).sum()\n",
    "        \n",
    "        # Jensen-Shannon divergence (a symmetric measure of similarity between distributions)\n",
    "        def kl_divergence(p, q):\n",
    "            # Add small epsilon to avoid division by zero\n",
    "            p = p + 1e-10\n",
    "            q = q + 1e-10\n",
    "            return np.sum(p * np.log(p / q))\n",
    "        \n",
    "        # Calculate the average distribution\n",
    "        m = (hist1_norm + hist2_norm) / 2\n",
    "        \n",
    "        # Calculate JS divergence\n",
    "        js_divergence = (kl_divergence(hist1_norm, m) + kl_divergence(hist2_norm, m)) / 2\n",
    "        \n",
    "        # Prepare results\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.update_progress(100, \"Preparing comparison results...\")\n",
    "        \n",
    "        results = {\n",
    "            'dataset1': dataset1_name,\n",
    "            'dataset2': dataset2_name,\n",
    "            'column': column,\n",
    "            'bins': bins,\n",
    "            'hist1': hist1,\n",
    "            'hist2': hist2,\n",
    "            'edges': edges1,  # We use edges1 since they should be the same for both histograms\n",
    "            'abs_difference': abs_diff,\n",
    "            'js_divergence': js_divergence,\n",
    "            'similarity_score': 1 - abs_diff  # Simple similarity score based on absolute difference\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        comparison_id = f\"{dataset1_name}_vs_{dataset2_name}_{column}\"\n",
    "        self.comparison_results[comparison_id] = results\n",
    "        \n",
    "        # Hide overlay\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.hide()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def value_counts_comparison(self, dataset1_name, dataset2_name, column):\n",
    "        \"\"\"\n",
    "        Compare value counts (categories) between two datasets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset1_name, dataset2_name : str\n",
    "            Names of datasets to compare\n",
    "        column : str\n",
    "            Column to compare\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Comparison results\n",
    "        \"\"\"\n",
    "        # Get the datasets\n",
    "        dataset1 = self.datasets.get(dataset1_name)\n",
    "        dataset2 = self.datasets.get(dataset2_name)\n",
    "        \n",
    "        if dataset1 is None or dataset2 is None:\n",
    "            raise ValueError(\"One or both datasets not found\")\n",
    "        \n",
    "        if column not in dataset1.columns or column not in dataset2.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in one or both datasets\")\n",
    "        \n",
    "        # Show processing overlay\n",
    "        self.overlay.show(overlay_type='spinner')\n",
    "        self.overlay.update_status(f\"Comparing value counts for column '{column}'\")\n",
    "        \n",
    "        # Get value counts\n",
    "        time.sleep(0.5)\n",
    "        counts1 = dataset1[column].value_counts().to_dict()\n",
    "        counts2 = dataset2[column].value_counts().to_dict()\n",
    "        \n",
    "        # Get all unique values from both datasets\n",
    "        all_values = set(counts1.keys()) | set(counts2.keys())\n",
    "        \n",
    "        # Create a comparison table\n",
    "        comparison = {}\n",
    "        for value in all_values:\n",
    "            count1 = counts1.get(value, 0)\n",
    "            count2 = counts2.get(value, 0)\n",
    "            diff = count2 - count1\n",
    "            pct_diff = (diff / count1 * 100) if count1 > 0 else float('inf')\n",
    "            \n",
    "            comparison[value] = {\n",
    "                'count_' + dataset1_name: count1,\n",
    "                'count_' + dataset2_name: count2,\n",
    "                'difference': diff,\n",
    "                'percent_difference': pct_diff\n",
    "            }\n",
    "        \n",
    "        # Prepare results\n",
    "        time.sleep(0.5)\n",
    "        results = {\n",
    "            'dataset1': dataset1_name,\n",
    "            'dataset2': dataset2_name,\n",
    "            'column': column,\n",
    "            'comparison_table': comparison,\n",
    "            'unique_values_dataset1': len(counts1),\n",
    "            'unique_values_dataset2': len(counts2),\n",
    "            'unique_values_both': len(all_values)\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        comparison_id = f\"{dataset1_name}_vs_{dataset2_name}_{column}_counts\"\n",
    "        self.comparison_results[comparison_id] = results\n",
    "        \n",
    "        # Hide overlay\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.hide()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_column_comparison(self, dataset1_name, dataset2_name, columns=None):\n",
    "        \"\"\"\n",
    "        Compare multiple columns between two datasets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset1_name, dataset2_name : str\n",
    "            Names of datasets to compare\n",
    "        columns : list, optional\n",
    "            List of columns to compare. If None, compares all common columns.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of comparison results for each column\n",
    "        \"\"\"\n",
    "        # Get the datasets\n",
    "        dataset1 = self.datasets.get(dataset1_name)\n",
    "        dataset2 = self.datasets.get(dataset2_name)\n",
    "        \n",
    "        if dataset1 is None or dataset2 is None:\n",
    "            raise ValueError(\"One or both datasets not found\")\n",
    "        \n",
    "        # Determine which columns to compare\n",
    "        if columns is None:\n",
    "            # Find common columns\n",
    "            columns = list(set(dataset1.columns) & set(dataset2.columns))\n",
    "        else:\n",
    "            # Verify all requested columns exist\n",
    "            for col in columns:\n",
    "                if col not in dataset1.columns or col not in dataset2.columns:\n",
    "                    raise ValueError(f\"Column '{col}' not found in one or both datasets\")\n",
    "        \n",
    "        # Show processing overlay with overall progress\n",
    "        self.overlay.show(overlay_type='progress', total=len(columns))\n",
    "        self.overlay.update_status(f\"Comparing {len(columns)} columns between datasets\")\n",
    "        \n",
    "        # Store batch results\n",
    "        batch_results = {}\n",
    "        \n",
    "        # Perform comparison for each column\n",
    "        for i, column in enumerate(columns):\n",
    "            self.overlay.update_progress(i, f\"Comparing column {i+1}/{len(columns)}: '{column}'\")\n",
    "            \n",
    "            # Determine the appropriate comparison method based on data type\n",
    "            if dataset1[column].dtype in [np.int64, np.float64] and dataset2[column].dtype in [np.int64, np.float64]:\n",
    "                # Numeric column - use distribution comparison\n",
    "                try:\n",
    "                    # Don't show the column overlay since we have a batch overlay\n",
    "                    result = self.column_distribution_comparison(dataset1_name, dataset2_name, column)\n",
    "                    batch_results[column] = {\n",
    "                        'type': 'distribution',\n",
    "                        'result': result\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    batch_results[column] = {\n",
    "                        'type': 'error',\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "            else:\n",
    "                # Categorical column - use value counts comparison\n",
    "                try:\n",
    "                    # Don't show the column overlay since we have a batch overlay\n",
    "                    result = self.value_counts_comparison(dataset1_name, dataset2_name, column)\n",
    "                    batch_results[column] = {\n",
    "                        'type': 'value_counts',\n",
    "                        'result': result\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    batch_results[column] = {\n",
    "                        'type': 'error',\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "        \n",
    "        # Complete the progress bar\n",
    "        self.overlay.update_progress(len(columns), \"Comparison complete!\")\n",
    "        time.sleep(0.5)\n",
    "        self.overlay.hide()\n",
    "        \n",
    "        return batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b64c70",
   "metadata": {},
   "source": [
    "## Apply Processing Overlay to Comparisons\n",
    "\n",
    "Now we'll integrate the processing overlay with the comparison operations to provide visual feedback during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample datasets for demonstration\n",
    "def generate_sample_datasets():\n",
    "    print(\"Generating sample datasets for comparison...\")\n",
    "    \n",
    "    # Dataset 1: Base dataset\n",
    "    df1 = pd.DataFrame({\n",
    "        'id': range(1, 1001),\n",
    "        'numeric_value': np.random.normal(50, 15, 1000),\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D'], 1000, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "        'binary_flag': np.random.choice([0, 1], 1000, p=[0.7, 0.3]),\n",
    "    })\n",
    "    \n",
    "    # Dataset 2: Similar to df1 but with some differences\n",
    "    df2 = pd.DataFrame({\n",
    "        'id': range(1, 1001),\n",
    "        'numeric_value': np.random.normal(55, 17, 1000),  # Slightly different distribution\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], 1000, p=[0.35, 0.25, 0.2, 0.1, 0.1]),  # Added a new category\n",
    "        'binary_flag': np.random.choice([0, 1], 1000, p=[0.6, 0.4]),  # Different proportion\n",
    "    })\n",
    "    \n",
    "    print(\"Sample datasets generated!\")\n",
    "    return df1, df2\n",
    "\n",
    "# Demonstrate the use of the comparator with overlay\n",
    "def run_comparison_demo():\n",
    "    # Generate sample datasets\n",
    "    df1, df2 = generate_sample_datasets()\n",
    "    \n",
    "    # Create comparator\n",
    "    comparator = DataComparator()\n",
    "    \n",
    "    # Add datasets with loading progress\n",
    "    print(\"Loading datasets into comparator...\")\n",
    "    overlay = ProcessingOverlay(\"Dataset Loading\")\n",
    "    overlay.show(overlay_type='progress', total=2)\n",
    "    \n",
    "    overlay.update_progress(0, \"Loading dataset 1...\")\n",
    "    time.sleep(1)  # Simulate loading time\n",
    "    comparator.add_dataset(\"original\", df1)\n",
    "    \n",
    "    overlay.update_progress(1, \"Loading dataset 2...\")\n",
    "    time.sleep(1)  # Simulate loading time\n",
    "    comparator.add_dataset(\"modified\", df2)\n",
    "    \n",
    "    overlay.update_progress(2, \"Datasets loaded successfully!\")\n",
    "    time.sleep(0.5)\n",
    "    overlay.hide()\n",
    "    \n",
    "    print(\"\\n1. Running single column distribution comparison...\")\n",
    "    dist_results = comparator.column_distribution_comparison(\"original\", \"modified\", \"numeric_value\", bins=15)\n",
    "    \n",
    "    print(\"\\n2. Running value counts comparison...\")\n",
    "    counts_results = comparator.value_counts_comparison(\"original\", \"modified\", \"category\")\n",
    "    \n",
    "    print(\"\\n3. Running batch comparison of all columns...\")\n",
    "    batch_results = comparator.batch_column_comparison(\"original\", \"modified\")\n",
    "    \n",
    "    return comparator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958fb9e",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Create visualizations of the comparison results after processing is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bf570",
   "metadata": {},
   "source": [
    "# Custom Comparison with Processing Overlay\n",
    "\n",
    "This notebook demonstrates how to implement a custom comparison operation with a loading/processing overlay to indicate when long-running operations are in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5a209",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import necessary libraries for data processing, visualization, and UI components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# IPython and Jupyter widgets for the overlay\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7ad71",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the dataset that will be used for comparison analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a63025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll create sample data\n",
    "# In a real scenario, you would load data from a file or API\n",
    "\n",
    "# Create sample dataset 1\n",
    "data1 = pd.DataFrame({\n",
    "    'ID': range(1, 1001),\n",
    "    'Value': np.random.normal(100, 15, 1000),\n",
    "    'Category': np.random.choice(['A', 'B', 'C', 'D'], 1000),\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=1000),\n",
    "    'Status': np.random.choice(['Active', 'Inactive', 'Pending'], 1000)\n",
    "})\n",
    "\n",
    "# Create sample dataset 2 (with some differences)\n",
    "data2 = pd.DataFrame({\n",
    "    'ID': range(1, 1001),\n",
    "    'Value': np.random.normal(105, 17, 1000),  # Slightly different distribution\n",
    "    'Category': np.random.choice(['A', 'B', 'C', 'D'], 1000),\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=1000),\n",
    "    'Status': np.random.choice(['Active', 'Inactive', 'Pending', 'New'], 1000)  # Added 'New' status\n",
    "})\n",
    "\n",
    "# Display sample of each dataset\n",
    "print(\"Dataset 1 Sample:\")\n",
    "display(data1.head())\n",
    "\n",
    "print(\"\\nDataset 2 Sample:\")\n",
    "display(data2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b416a1",
   "metadata": {},
   "source": [
    "## Basic Data Processing\n",
    "\n",
    "Prepare and clean the data for the comparison operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess datasets before comparison\n",
    "def preprocess_data(df, name):\n",
    "    \"\"\"\n",
    "    Perform basic preprocessing on the dataframe\n",
    "    - Handle missing values\n",
    "    - Standardize column types\n",
    "    - Create a copy to avoid modifying the original\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Fill any missing values\n",
    "    processed_df.fillna({\n",
    "        'Value': processed_df['Value'].mean(),\n",
    "        'Category': 'Unknown',\n",
    "        'Status': 'Unknown'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Ensure consistent data types\n",
    "    processed_df['ID'] = processed_df['ID'].astype(int)\n",
    "    processed_df['Value'] = processed_df['Value'].astype(float)\n",
    "    processed_df['Date'] = pd.to_datetime(processed_df['Date'])\n",
    "    \n",
    "    # Add source indicator\n",
    "    processed_df['Source'] = name\n",
    "    \n",
    "    print(f\"Preprocessing complete for {name}\")\n",
    "    return processed_df\n",
    "\n",
    "# Preprocess both datasets\n",
    "data1_processed = preprocess_data(data1, \"Dataset 1\")\n",
    "data2_processed = preprocess_data(data2, \"Dataset 2\")\n",
    "\n",
    "# Display summaries\n",
    "print(\"\\nDataset 1 Summary:\")\n",
    "display(data1_processed.describe())\n",
    "\n",
    "print(\"\\nDataset 2 Summary:\")\n",
    "display(data2_processed.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af7edf",
   "metadata": {},
   "source": [
    "## Custom Comparison with Processing Overlay\n",
    "\n",
    "Implement a custom comparison function with a loading/processing overlay to indicate when long-running operations are in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loading overlay widget\n",
    "def create_loading_widget(message=\"Processing...\"):\n",
    "    \"\"\"Create a loading widget with a message and spinner\"\"\"\n",
    "    spinner = widgets.HTML(\n",
    "        value='<i class=\"fa fa-spinner fa-spin\" style=\"font-size:24px;color:blue\"></i>',\n",
    "        layout=widgets.Layout(margin='0 10px 0 0')\n",
    "    )\n",
    "    text = widgets.HTML(value=f\"<h3>{message}</h3>\")\n",
    "    return widgets.HBox([spinner, text])\n",
    "\n",
    "# Function to perform comparison with an overlay\n",
    "def compare_datasets_with_overlay(df1, df2, id_column='ID'):\n",
    "    \"\"\"\n",
    "    Compare two datasets with a loading overlay\n",
    "    Returns a comparison result dataframe\n",
    "    \"\"\"\n",
    "    # Create and display loading widget\n",
    "    loading_widget = create_loading_widget(\"Comparing datasets. Please wait...\")\n",
    "    display(loading_widget)\n",
    "    \n",
    "    # Initialize empty comparison results\n",
    "    comparison_results = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Simulate a long-running process\n",
    "        for step in tqdm(range(5), desc=\"Comparison Progress\"):\n",
    "            if step == 0:\n",
    "                # Check for matching IDs\n",
    "                ids_in_df1_not_df2 = set(df1[id_column]) - set(df2[id_column])\n",
    "                ids_in_df2_not_df1 = set(df2[id_column]) - set(df1[id_column])\n",
    "                common_ids = set(df1[id_column]).intersection(set(df2[id_column]))\n",
    "                \n",
    "                print(f\"IDs in Dataset 1 but not in Dataset 2: {len(ids_in_df1_not_df2)}\")\n",
    "                print(f\"IDs in Dataset 2 but not in Dataset 1: {len(ids_in_df2_not_df1)}\")\n",
    "                print(f\"Common IDs: {len(common_ids)}\")\n",
    "                \n",
    "                time.sleep(1)  # Simulate processing time\n",
    "                \n",
    "            elif step == 1:\n",
    "                # Compare numerical differences for common IDs\n",
    "                # Filter both dataframes to include only common IDs\n",
    "                df1_common = df1[df1[id_column].isin(common_ids)]\n",
    "                df2_common = df2[df2[id_column].isin(common_ids)]\n",
    "                \n",
    "                # Analyze numerical columns\n",
    "                num_cols = df1_common.select_dtypes(include=['number']).columns\n",
    "                num_cols = [col for col in num_cols if col != id_column]\n",
    "                \n",
    "                for col in num_cols:\n",
    "                    if col in df2_common.columns:\n",
    "                        df1_common = df1_common.set_index(id_column)\n",
    "                        df2_common = df2_common.set_index(id_column)\n",
    "                        \n",
    "                        # Calculate differences\n",
    "                        diff = df1_common[col] - df2_common[col]\n",
    "                        pct_diff = ((df1_common[col] - df2_common[col]) / df2_common[col]) * 100\n",
    "                        \n",
    "                        # Add to comparison results\n",
    "                        if comparison_results.empty:\n",
    "                            comparison_results = pd.DataFrame(index=diff.index)\n",
    "                        \n",
    "                        comparison_results[f\"{col}_diff\"] = diff\n",
    "                        comparison_results[f\"{col}_pct_diff\"] = pct_diff\n",
    "                \n",
    "                time.sleep(1)  # Simulate processing time\n",
    "                \n",
    "            elif step == 2:\n",
    "                # Compare categorical columns\n",
    "                cat_cols = df1.select_dtypes(include=['object']).columns\n",
    "                cat_cols = [col for col in cat_cols if col != id_column and col != 'Source']\n",
    "                \n",
    "                for col in cat_cols:\n",
    "                    if col in df2.columns:\n",
    "                        df1_common = df1[df1[id_column].isin(common_ids)].set_index(id_column)\n",
    "                        df2_common = df2[df2[id_column].isin(common_ids)].set_index(id_column)\n",
    "                        \n",
    "                        # Flag mismatched values\n",
    "                        match_status = (df1_common[col] == df2_common[col])\n",
    "                        \n",
    "                        # Add to comparison results\n",
    "                        if comparison_results.empty:\n",
    "                            comparison_results = pd.DataFrame(index=match_status.index)\n",
    "                        \n",
    "                        comparison_results[f\"{col}_match\"] = match_status\n",
    "                        \n",
    "                        # Add the actual values for comparison\n",
    "                        comparison_results[f\"{col}_df1\"] = df1_common[col]\n",
    "                        comparison_results[f\"{col}_df2\"] = df2_common[col]\n",
    "                \n",
    "                time.sleep(1)  # Simulate processing time\n",
    "                \n",
    "            elif step == 3:\n",
    "                # Compare distributions\n",
    "                num_cols = df1.select_dtypes(include=['number']).columns\n",
    "                num_cols = [col for col in num_cols if col != id_column]\n",
    "                \n",
    "                distribution_summary = {}\n",
    "                \n",
    "                for col in num_cols:\n",
    "                    if col in df2.columns:\n",
    "                        # Basic statistics\n",
    "                        stats_df1 = df1[col].describe()\n",
    "                        stats_df2 = df2[col].describe()\n",
    "                        \n",
    "                        # Store statistics in the summary\n",
    "                        distribution_summary[col] = {\n",
    "                            'mean_diff': stats_df1['mean'] - stats_df2['mean'],\n",
    "                            'std_diff': stats_df1['std'] - stats_df2['std'],\n",
    "                            'min_df1': stats_df1['min'],\n",
    "                            'min_df2': stats_df2['min'],\n",
    "                            'max_df1': stats_df1['max'],\n",
    "                            'max_df2': stats_df2['max'],\n",
    "                            'distribution_shift': abs(stats_df1['mean'] - stats_df2['mean']) / stats_df1['std']\n",
    "                        }\n",
    "                \n",
    "                distribution_df = pd.DataFrame(distribution_summary).T\n",
    "                \n",
    "                time.sleep(1)  # Simulate processing time\n",
    "                \n",
    "            elif step == 4:\n",
    "                # Finalize and add summary stats\n",
    "                if not comparison_results.empty:\n",
    "                    # Reset index to make ID a column again\n",
    "                    comparison_results = comparison_results.reset_index().rename(columns={'index': id_column})\n",
    "                    \n",
    "                    # Count total differences\n",
    "                    match_columns = [col for col in comparison_results.columns if col.endswith('_match')]\n",
    "                    if match_columns:\n",
    "                        comparison_results['total_mismatches'] = len(match_columns) - comparison_results[match_columns].sum(axis=1)\n",
    "                \n",
    "                time.sleep(1)  # Simulate processing time\n",
    "        \n",
    "        # Clear the loading widget and display results\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Show summary\n",
    "        print(f\"Comparison complete! Found differences in {len(comparison_results[comparison_results['total_mismatches'] > 0])} records.\")\n",
    "        \n",
    "        return comparison_results, distribution_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Clear the loading widget and show error\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Error during comparison: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Ensure loading widget is cleared\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# Run the comparison with overlay\n",
    "comparison_results, distribution_summary = compare_datasets_with_overlay(data1_processed, data2_processed)\n",
    "\n",
    "# Display the results\n",
    "print(\"Comparison Results (showing records with differences):\")\n",
    "display(comparison_results[comparison_results['total_mismatches'] > 0].head(10))\n",
    "\n",
    "print(\"\\nDistribution Differences Summary:\")\n",
    "display(distribution_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b83d5",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "Create visualizations to display the comparison results once processing is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303785af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for better visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Function to create visualizations for comparison results\n",
    "def visualize_comparison_results(comparison_df, dist_summary, df1, df2):\n",
    "    \"\"\"Create various visualizations for the comparison results\"\"\"\n",
    "    \n",
    "    # Create loading widget for visualizations\n",
    "    loading_widget = create_loading_widget(\"Generating visualizations. Please wait...\")\n",
    "    display(loading_widget)\n",
    "    \n",
    "    try:\n",
    "        # 1. Distribution of mismatches\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(comparison_df['total_mismatches'], kde=True)\n",
    "        plt.title('Distribution of Mismatches per Record')\n",
    "        plt.xlabel('Number of Mismatches')\n",
    "        plt.ylabel('Count')\n",
    "        time.sleep(1)  # Simulate processing time\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Comparison of Value distributions\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Combine data for side-by-side comparison\n",
    "        combined_data = pd.concat([\n",
    "            df1[['Value', 'Source']],\n",
    "            df2[['Value', 'Source']]\n",
    "        ])\n",
    "        \n",
    "        # Create the violin plot\n",
    "        sns.violinplot(x='Source', y='Value', data=combined_data)\n",
    "        plt.title('Comparison of Value Distributions')\n",
    "        plt.tight_layout()\n",
    "        time.sleep(1)  # Simulate processing time\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Bar chart of category distributions\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        # Count categories in each dataset\n",
    "        cat_counts1 = df1['Category'].value_counts().reset_index()\n",
    "        cat_counts1.columns = ['Category', 'Count']\n",
    "        cat_counts1['Source'] = 'Dataset 1'\n",
    "        \n",
    "        cat_counts2 = df2['Category'].value_counts().reset_index()\n",
    "        cat_counts2.columns = ['Category', 'Count']\n",
    "        cat_counts2['Source'] = 'Dataset 2'\n",
    "        \n",
    "        # Combine counts\n",
    "        combined_cats = pd.concat([cat_counts1, cat_counts2])\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        sns.barplot(x='Category', y='Count', hue='Source', data=combined_cats)\n",
    "        plt.title('Category Distribution Comparison')\n",
    "        plt.tight_layout()\n",
    "        time.sleep(1)  # Simulate processing time\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # 4. Status distribution comparison\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        # Count statuses in each dataset\n",
    "        status_counts1 = df1['Status'].value_counts().reset_index()\n",
    "        status_counts1.columns = ['Status', 'Count']\n",
    "        status_counts1['Source'] = 'Dataset 1'\n",
    "        \n",
    "        status_counts2 = df2['Status'].value_counts().reset_index()\n",
    "        status_counts2.columns = ['Status', 'Count']\n",
    "        status_counts2['Source'] = 'Dataset 2'\n",
    "        \n",
    "        # Combine counts\n",
    "        combined_status = pd.concat([status_counts1, status_counts2])\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        sns.barplot(x='Status', y='Count', hue='Source', data=combined_status)\n",
    "        plt.title('Status Distribution Comparison')\n",
    "        plt.tight_layout()\n",
    "        time.sleep(1)  # Simulate processing time\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # 5. Correlation of differences in numerical values\n",
    "        if 'Value_diff' in comparison_df.columns:\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            \n",
    "            # Create a scatter plot of original values vs differences\n",
    "            plt.scatter(\n",
    "                df1.set_index('ID').loc[comparison_df['ID'], 'Value'],\n",
    "                comparison_df['Value_diff'],\n",
    "                alpha=0.5\n",
    "            )\n",
    "            plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "            plt.title('Value Differences vs Original Values')\n",
    "            plt.xlabel('Original Value (Dataset 1)')\n",
    "            plt.ylabel('Difference (Dataset 1 - Dataset 2)')\n",
    "            plt.tight_layout()\n",
    "            time.sleep(1)  # Simulate processing time\n",
    "            clear_output(wait=True)\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"Visualization complete!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Error during visualization: {str(e)}\")\n",
    "    finally:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# Run the visualization function\n",
    "visualize_comparison_results(comparison_results, distribution_summary, data1_processed, data2_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63452db3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented a custom comparison function with a loading/processing overlay that:\n",
    "\n",
    "1. Compares two datasets across multiple dimensions (numerical differences, categorical mismatches)\n",
    "2. Shows a progress indicator during the comparison process\n",
    "3. Creates visualizations to help understand the differences between datasets\n",
    "4. Provides a summary of the differences found\n",
    "\n",
    "This approach is particularly useful for:\n",
    "- Large datasets where comparisons might take significant time\n",
    "- Datasets with complex structures that require multiple comparison steps\n",
    "- Situations where users need feedback on processing status\n",
    "- Analysis requiring visual interpretation of differences\n",
    "\n",
    "Further improvements could include:\n",
    "- Adding more sophisticated statistical comparisons\n",
    "- Implementing interactive widgets for exploring differences\n",
    "- Adding export options for the comparison results\n",
    "- Optimizing the comparison algorithm for very large datasets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
